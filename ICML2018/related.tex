\section{Related Work}
Our work is inspired by the following three literatures 

\emph{Peer Prediction:} This line of works, addressing the incentive issues for reporting high quality data without verification, starts roughly with the seminar works \cite{prelec2004bayesian,gneiting2007strictly}. A series of follow-up works have relaxed various assumptions of this family of mechanisms \cite{jurca2009mechanisms,witkowski2012peer,radanovic2013robust,dasgupta2013crowdsourced}. 

\emph{Inference method:} Recently, Inference method have been applied to crowdsourcing settings, aiming to uncover the true labels from multiple noisy reported copies. Notably success include EM method \cite{dawid1979maximum,raykar2010learning,zhang2014spectral}, Variational Inference \cite{liu2012variational,chen2015statistical} and Minimax Entropy Inference~\cite{zhou2012learning,zhou2014aggregating}. Besides, \citet{zheng2017truth} provide a good survey of the existing inference algorithms.

\emph{Reinforcement Learning:} In RL problems, an agent interacts with an unknown environment and attempts to maximize its cumulative utility \citep{Sutton98,Szepesvari10}. Recently, various RL aglorithms have been developed and sucessfully applied to the interaction with human beings~\cite{engel2005reinforcement,gasic2014gaussian}.

Our work differs from the above literature in the connection between incentive mechanisms with ML. There have been very few recenct studies that have similar taste as ours.
For example, to improve the utility of the data requester in crowdsourcing, \citet{liu2017sequential} develop a multi-armed bandit algorithm to adjust the state-of-the-art peer prediction mechanism, DG13~\cite{dasgupta2013crowdsourced}.
However, both the bandit algorithm and DG13 still need to assume that workers are fully rational.
Instead of randomly choosing a reference worker, \citet{liu2017machine} propose to use supervised learning algorithms to generate the reference reports based on the contextual information of tasks and derive the corresponding IC conditions.
In this paper, without assuming the contextual information about tasks, we use Bayesian inference to learn workers' states and true labels, which is an unsupervised-learning algorithm.
%Our payments are determined based on the learned worker models, and we derive the incentive compatibility conditions for our unsupervised-learning-based mechanism.

%This idea has also been s\citet{RMDE,cai2018reinforcement} propose to build incentive mechanisms based on reinforcement learning.
%However, focusing on the empirical analysis, they never consider the theoretical incentive compatibility.
%In this paper, we also incorporate reinforcement learning to get rid of the assumption that workers are fully rational.
%When analyzing our incentive mechanism, we go one-step further by not only providing the empirical analysis but also present a novel proof for the incentive compatibility related with reinforcement learning.


%Peer prediction \cite{Prelec:2004,MRZ:2005,jurca2006minimum,jurca2009mechanisms,witkowski2012robust,witkowski2012peer,radanovic2013,dasgupta2013crowdsourced}, a class of mechanisms that have been developed extensively recently, are often adopted for incentivizing truthful or high-quality contributions from strategic sources when the quality of the contributions cannot be verified. %The Bayesian Truth Serum was proposed in \cite{Prelec:2004} to elicit truthful report when only minority of the crowd holds the true answer. 
%Particularly, the seminal work \cite{MRZ:2005} established that strictly proper scoring rule \cite{Gneiting:07} can be adopted in the peer prediction setting for eliciting truthful reports; following which a sequence of followed up works have been done on relaxing the assumptions that have been imposed therein \cite{witkowski2012robust,witkowski2012peer,radanovic2013}. More recently, \cite{Witkowski_hcomp13,dasgupta2013crowdsourced} formally introduced and studied an effort sensitive model for binary signal data elicitation. Particularly \cite{dasgupta2013crowdsourced} proposed a multi-task peer prediction mechanism that can help remove undesirable equilibria that lead to low quality reports. These results are further strengthened and extended to a non-binary signal setting in \cite{2016arXiv160303151S,kong2016framework}. One set of studies focuses on using models that aggregate tasks from different agents \cite{kamar2012incentives,faltings2014incentive,radanovic2016incentives}. These work do not consider a machine learning setting, and have not established a rigorous analysis. 
%
%
%
%[add inference and reinforcement learning]