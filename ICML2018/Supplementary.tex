%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{enumitem}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
%\makeatletter
%\renewenvironment{proof}[1][\proofname]{\par
%  \vspace{-\topsep}% remove the space after the theorem
%  \pushQED{\qed}%
%  \normalfont
%  \topsep0pt \partopsep0pt % no space before
%  \trivlist
%  \item[\hskip\labelsep
%        \itshape
%    #1\@addpunct{.}]\ignorespaces
%}{%
%  \popQED\endtrivlist\@endpefalse
%  \addvspace{0pt plus 0pt} % some space after
%}
%\makeatother

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Supplementary File}

\begin{document}

\twocolumn[
\icmltitle{Supplementary File for\\Incentivizing High Quality Crowdsourcing Information using Bayesian Inference and Reinforcement Learning} 

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zehong Hu}{NTU}
\icmlauthor{Yang Liu}{Harvard}
\icmlauthor{Yitao Liang}{UCLA}
\icmlauthor{Jie Zhang}{NTU}
\end{icmlauthorlist}

%\icmlaffiliation{NTU}{Rolls-Royce Cooperate Lab@NTU, School of Computer Science and Engineering, Nanyang Technological University, Singapore}
%\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
%\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

%\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
%\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
%\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{} % otherwise use the standard text.
\section{Basic Lemmas}
\begin{lemma}
\label{MoGene}
If $x\sim \mathrm{Bin}(n,p)$, $\mathbb{E}t^x= \left(1-p+tp\right)^{n}$ holds for any $t>0$, where $\mathrm{Bin}(\cdot)$ is the binomial distribution.
\begin{proof}
\begin{equation}
t^x = e^{x\log t}=m_x(\log t)= \left(1-p+pe^{\log t}\right)^{n}
\end{equation}
where $m_x(\cdot)$ denotes the moment generating function.
\end{proof}
\end{lemma}

\begin{lemma}
\label{SolveF}
For given $n,m\geq 0$, if $0\leq p\leq 1$, we can have
\begin{equation*}
\begin{split}
&{\sum}_{x=0}^{n}{\sum}_{w=0}^{m} C_{n}^{x}C_{m}^{w}p^{x+w}(1-p)^{y+z}\times\\
&\qquad\qquad\qquad B(x+z+1+t,y+w+1)=\\
&\int_{0}^{1}[(2p-1)x+1-p]^{n}[(1-2p)x+p]^{m}x^{t}\mathrm{d}x
\end{split}
\end{equation*}
\begin{proof}
By the definition of the beta function~\cite{olver2010nist},
\begin{equation}
B(x, y) = \int_{0}^{+\infty} u^{x-1}(1+u)^{-(x+y)}\mathrm{d}u
\end{equation}
we can have
\begin{align}
&\sum_{x,w} C_{n}^{x}C_{m}^{w}p^{x+w}(1-p)^{y+z}B(x+z+1+t,y+w+1)\nonumber\\
&= \int_{0}^{+\infty} \mathbb{E}u^{x}\cdot\mathbb{E}u^z \cdot u^t\cdot (1+u)^{-(n+m+2+t)}\mathrm{d}u
\end{align}
where we regard $x\sim \mathrm{Bin}(n,p)$ and $z\sim \mathrm{Bin}(m,1-p)$.
Thus, according to Lemma~\ref{MoGene}, we can obtain
\begin{equation}
\begin{split}
&\int_{0}^{+\infty} \mathbb{E}u^{x}\cdot\mathbb{E}u^z \cdot u^t\cdot (1+u)^{-(n+m+3)}\mathrm{d}u\\
&=\int_{0}^{+\infty} \frac{[1-p+up]^n\cdot [p+(1-p)u]^m\cdot u^t}{(1+u)^{n+m+2+t}}\mathrm{d}u.
\end{split}
\end{equation}
For the integral operation, substituting $u$ with $v-1$ at first and then $v$ with $(1-x)^{-1}$, we can conclude Lemma~\ref{SolveF}.
\end{proof}
\end{lemma}

\begin{lemma}
\label{Sum1}
$\sum_{n=0}^{N}C_N^{n}\cdot x^{n}=(1+x)^N$.
\end{lemma}
\begin{lemma}
\label{Sum2}
$\sum_{n=0}^{N} C_N^{n}\cdot n\cdot x^{n}=N\cdot x\cdot (1+x)^{N-1}$.
\end{lemma}
\begin{lemma}
\label{Sum3}
$\sum_{n=0}^{N} C_N^{n}\cdot n\cdot x^{N-n}=N\cdot (1+x)^{N-1}$.
\end{lemma}
\begin{lemma}
\label{Sum5}
$\sum_{n=0}^{N} C_N^{n}\cdot n^2\cdot x^{n}=Nx(1+Nx)(1+x)^{N-2}$.
\end{lemma}
\begin{lemma}
\label{Sum6}
$\sum_{n=0}^{N} C_N^{n}\cdot n^2\cdot x^{N-n}=N(x+N)(1+x)^{N-2}$.
\end{lemma}
\begin{lemma}
\label{Sum4}
If $0<x<1$, we can have
\begin{equation*}
\begin{split}
\sum_{n=0}^{\lfloor N/2\rfloor} C_N^{n}\cdot x^{n} &\geq \left(1-e^{-cN}\right) \cdot (1+x)^{N}\\
\sum_{n=\lfloor N/2 \rfloor +1}^{N} C_N^{n}\cdot x^{N-n}&\geq \left(1-e^{-cN}\right) \cdot (1+x)^{N}.
\end{split}
\end{equation*}
where $c=0.5(1-x)^2(1+x)^{-2}$.
\begin{proof}
To prove the lemmas above, we firstly define
\begin{equation}
F_t(x)=\sum_{n=0}^{N} C_N^{n} n^t x^{n}
\end{equation}
Then, Lemma~\ref{Sum1} can be obtained by expanding $(1+x)^N$.
Lemma~\ref{Sum2} can be proved as follows
\begin{equation}
\begin{split}
F_1(x)&=\sum_{n=0}^{N} C_N^{n} (n+1) x^{n}-(1+x)^N\\
\sum_{n=0}^{N} C_N^{n} (n+1) x^{n}&=\frac{\mathrm{d}}{\mathrm{d}x}\left[xF_0(x)\right]\\
&=Nx(1+x)^{N-1}+(1+x)^N.
\end{split}
\end{equation}
Lemma~\ref{Sum3} can be obtained as follows
\begin{equation}
\begin{split}
\sum_{n=0}^{N} C_N^{n} n x^{N-n}&=x^N\sum_{n=0}^{N} C_N^{n} n \left(\frac{1}{x}\right)^n\\
&=x^N\cdot N\cdot \frac{1}{x}\cdot \left(1+\frac{1}{x}\right)^{N-1}.
\end{split}
\end{equation}
For Lemma~\ref{Sum5}, we can have
\begin{align}
F_2(x)&=\sum_{n=0}^{N} C_N^{n} (n+2)(n+1) x^{n}-3F_{1}(x)-2F_{0}(x)\nonumber \\
&=\left[x^2F_0(x)\right]' -3F_{1}(x)-2F_{0}(x)
\end{align}
Thus, we can have
\begin{equation}
F_2(x)=Nx(1+Nx)(1+x)^{N-2}
\end{equation}
which concludes Lemma~\ref{Sum5}.
Then, Lemma~\ref{Sum6} can be obtained by considering Equation~\ref{Eqv}.
\begin{equation}
\label{Eqv}
\begin{split}
\sum_{n=0}^{N} C_N^{n} n^2 x^{N-n}=x^N\sum_{n=0}^{N} C_N^{n} n^2 \left(\frac{1}{x}\right)^n.
\end{split}
\end{equation}
For Lemma~\ref{Sum4}, we can have
\begin{equation}
\sum_{n=0}^{\lfloor N/2\rfloor} C_N^{n} x^{n}=(1+x)^{N}\sum_{n=0}^{\lfloor N/2\rfloor} C_N^{n} p^n (1-p)^{N-n}
\end{equation}
where $p=x(1+x)^{-1}$. Let $X\sim \mathrm{Bin}(N, p)$, we can have
\begin{equation}
\sum_{n=0}^{\lfloor N/2\rfloor} C_N^{n} p^n (1-p)^{N-n}\geq 1-P\left(X\geq N/2\right).
\end{equation}
Since $x<1$, $p<0.5$ and $Np<N/2$. Considering Hoeffding's inequality, we can get
\begin{equation}
P\left(X\geq N/2\right)\leq \exp \left[-\frac{N(1-x)^2}{2(1+x)^2}\right]
\end{equation}
which concludes the first inequality in Lemma~\ref{Sum4}. Similarly, for the second inequality, we can have
\begin{equation}
\sum_{n=K}^{N} C_N^{n}x^{N-n}=(1+x)^{N}\sum_{n=K}^{N} C_N^{n} (1-p)^n p^{N-n}
\end{equation}
where $K=\lfloor N/2 \rfloor +1$. Suppose $Y\sim \mathrm{Bin}(N, 1-p)$, we can have
\begin{equation}
\sum_{n=K}^{N} C_N^{n} (1-p)^n p^{N-n}\geq 1-P\left(Y\leq N/2\right).
\end{equation}
Considering Hoeffding's inequality, we can also get
\begin{equation}
P\left(Y\leq N/2\right)\leq \exp \left[-\frac{N(1-x)^2}{2(1+x)^2}\right]
\end{equation}
which concludes the second inequality in Lemma~\ref{Sum4}.
\end{proof}
\end{lemma}

\begin{lemma}
\label{Inequality1}
For any $x,y\geq 0$, we can have
$$(1+x)^{y}\leq e^{xy}.$$
\begin{proof}
Firstly, we can know $(1+x)^{y}=e^{y\log(1+x)}$. Let $f(x)=x-\log(x)$. Then, we can have $f(0)=0$ and $f'(x)\geq 0$. Thus, $x\geq\log (1+x)$ and we can conclude Lemma~\ref{Inequality1} by taking this inequality into the equality.
\end{proof}
\end{lemma}

\begin{lemma}
\label{Concave1}
$$g(x)=\frac{e^x}{e^x+1}$$
is a concave function when $x\in [0,+\infty)$.
\begin{proof}
$g'(x)= (2+t(x))^{-1}$, where $t(x)=e^x+e^{-x}$. $t'(x)=e^x-e^{-x}\geq 0$ when $x\in [0,+\infty)$. Thus, $g'(x)$ is monotonically decreasing when $x\in [0,+\infty)$, which concludes Lemma~\ref{Concave1}.
\end{proof}
\end{lemma}

\begin{lemma}
\label{Concave2}
For $x\in(-\infty, +\infty)$, 
$$h(x)=\frac{1}{e^{|x|}+1}$$
satisfies
$$h(x)< e^x \;\;\mathrm{and}\;\; h(x)< e^{-x}.$$
\begin{proof}
When $x\geq 0$, we can have
\begin{equation}
h(x)<\frac{1}{e^x}=e^{-x}\leq e^{x}.
\end{equation}
When $x\leq 0$, we can have
\begin{equation}
h(x)=\frac{e^x}{e^x+1}<e^{x}\leq e^{-x}.
\end{equation}
\end{proof}
\end{lemma}

\begin{lemma}
\label{Concave3}
If $\lambda=p/(1-p)$ and $0.5<p<1$, then
$${\sum}_{n=\lfloor N/2 \rfloor}^{N}C_N^n\lambda^{m-n}p^n(1-p)^m\leq [4p(1-p)]^{N/2}$$
$${\sum}_{n=0}^{\lfloor N/2 \rfloor}C_N^n\lambda^{n-m}p^n(1-p)^m\leq [4p(1-p)]^{N/2}$$
where $m=N-n$.
\begin{proof}
For the first inequality, we can have
\begin{align}
&\sum_{n=\lfloor N/2 \rfloor}^{N}C_N^n\lambda^{m-n}p^n(1-p)^m \\&=\sum_{n=\lfloor N/2 \rfloor}^{N}C_N^np^m(1-p)^n\leq \sum_{m=0}^{\lfloor N/2 \rfloor}C_N^m p^m(1-p)^n\nonumber
\end{align}
According to the inequality in \cite{Arratia1989}, we can have
\begin{equation}
\label{xxx}
 \sum_{m=0}^{\lfloor N/2 \rfloor}C_N^m p^m(1-p)^n \leq \exp (-ND)
\end{equation}
where $D=-0.5\log(2p)-0.5\log(2(p-1))$, which concludes the first inequality in Lemma~\ref{Concave3}.

For the second inequality, we can have
\begin{equation}
\begin{split}
&\sum_{n=0}^{\lfloor N/2 \rfloor}C_N^n\lambda^{n-m}p^n(1-p)^m \\
&= \frac{1}{[p(1-p)]^N}\sum_{n=0}^{\lfloor N/2 \rfloor}C_N^n[p^3]^n[(1-p)^3]^m
 \\&=\frac{[p^3+(1-p)^3]^N}{[p(1-p)]^N}\sum_{n=0}^{\lfloor N/2 \rfloor}C_N^n x^n(1-x)^m
\end{split}
\end{equation}
where $x=p^3/[p^3+(1-p)^3]$. By using Equation~\ref{xxx}, we can have
\begin{equation}
\begin{split}
&\sum_{n=0}^{\lfloor N/2 \rfloor}C_N^n\lambda^{n-m}p^n(1-p)^m\\
&\leq \frac{[p^3+(1-p)^3]^N}{[p(1-p)]^N} [x(1-x)]^{N/2} \\
&=[4p(1-p)]^{N/2}
\end{split}
\end{equation}
which concludes the second inequality of Lemma~\ref{Concave3}.
\end{proof}
\end{lemma}


\section{H function analysis}
Here, we present our analysis on the $H$ function defined in the proof of Proposition~1.
Firstly, we can have:
\begin{lemma}
\label{MidPoint}
$H(m,0.5;M,t) = 2(t+1)^{-1}$.
\end{lemma}
\begin{lemma}
\label{Symmetry}
$H(m,p;M,t) = H(n, \bar{p};M,t)$.
\end{lemma}
\begin{lemma}
\label{LogConvexity}
As a function of $m$, $H(m,p;M,t)$ is logarithmically convex.
\begin{proof}
Lemma~\ref{MidPoint} can be proved by integrating $2 x^t$ on $[0,1]$.
Lemma~\ref{Symmetry} can be proved by showing that $H(n, \bar{p};M,t)$ has the same expression as $H(m,p;M,t)$.
Thus, in the following proof, we focus on Lemma~\ref{LogConvexity}. Fixing $p$, $M$ and $t$, we denote $\log (H)$ by $f(m)$. Then, we compute the first-order derivative as
\begin{equation}
H(m)f'(m)=2^{M+1}\int_{0}^{1}\lambda u^{n}(1-u)^{m}x^t\mathrm{d}x
\end{equation}
where $u= (2p-1)x+1-p$ and $\lambda=\log(1-u)-\log(u)$. Furthermore, we can solve the second-order derivative as
\begin{equation}
\begin{split}
&2^{-2(M+1)}H^2(m)f''(m)=\\
&\int_{0}^{1}g^2(x)\mathrm{d}x\int_{0}^{1}h^2(x)\mathrm{d}x-\left(\int_{0}^{1}g(x)h(x)\mathrm{d}x\right)^2
\end{split}
\end{equation}
where the functions $g,h:(0,1)\rightarrow  \mathbb{R}$ are defined by
\begin{equation}
g=\lambda\sqrt{u^{n}(1-u)^{m}}\;, \; h = \sqrt{u^{n}(1-u)^{m}}.
\end{equation}
By the Cauchy-Schwarz inequality,
\begin{equation}
\int_{0}^{1}g^2(x)\mathrm{d}x\int_{0}^{1}h^2(x)\mathrm{d}x\geq \left(\int_{0}^{1}g(x)h(x)\mathrm{d}x\right)^2
\end{equation}
we can know that $f''(m)\geq 0$ always holds, which concludes that $f$ is convex and $H$ is logarithmically convex.
\end{proof}
\end{lemma}
Then, for the case that $t=1$ and $M\gg 1$, we can further derive the following three lemmas for $H(m,p;M,1)$:
\begin{lemma}
\label{Ratio1}
The ratio between two ends satisfies
$$\log\frac{H(0,p;M,1)}{H(M,p;M,1)} \approx \left\{
    \begin{array}{cl}
    \log(M)+\epsilon(p) & p>0.5\\
    0 & p=0.5\\
    -\log(M)-\epsilon(\bar{p}) & p<0.5
    \end{array}\right.$$
\end{lemma}
where $\epsilon(p)=\log(2p-1)-\log(p)$ and $\epsilon(p)=0$ if $p=0.5$.
\begin{lemma}
\label{LowBound1}
The lower bound can be calculated as
\begin{equation*}
\log H(m,p)\gtrsim \left\{
    \begin{array}{cl}
    H(0,p)- \underline{k}m& 2m\leq M\\
    H(M,p)- \underline{k}n& 2m>M
    \end{array}\right.
\end{equation*}
where $\underline{k}=\log\left(\max\left\{p/(\bar{p}+M^{-1}),\bar{p}/(p+M^{-1})\right\}\right)$.
\end{lemma}
\begin{lemma}
\label{UpBound1}
The upper bound can be calculated as
\begin{equation*}
\log H(m,p)\lesssim \left\{
    \begin{array}{cl}
    H(0,p)- \overline{k}m& 2m\leq M\\
    H(M,p)- \overline{k}n& 2m>M
    \end{array}\right.
\end{equation*}
where $n=M-m$ and $\overline{k}=2\log\left(2\cdot \max\left\{p,\bar{p}\right\}\right)$.
\begin{proof}
By Lemma~\ref{MidPoint}, $\log H(m, 0.5;M,1)\equiv 0$, which proves the above three lemmas for the case that $p=0.5$. Considering the symmetry ensured by Lemma~\ref{Symmetry}, we thus focus on the case that $p>0.5$ in the following proof and transform $H(m,p)$ into the following formulation
\begin{equation}
H(m,p)=\omega(p)\cdot \int_{\bar{p}}^{p}x^n(1-x)^m(x-1+p)\mathrm{d}x
\end{equation}
where $\omega(p)=2^{M+1}/(2p-1)^2$. Then, we can solve $H(0,p)$ and $H(M,p)$ as
\begin{equation}
\begin{split}
H(0,p)&=\omega(p)\int_{\bar{p}}^{p}x^M(x-\bar{p})\mathrm{d}x\\
&=\frac{(2p)^{M+1}}{(2p-1)(M+1)} - O\left(\frac{(2p)^{M+1}}{M^2}\right)
\end{split}
\end{equation}
\begin{equation}
\begin{split}
&H(M,p)=\omega(p)\int_{\bar{p}}^{p}(1-x)^M(x-\bar{p})\mathrm{d}x\\
&=\frac{p(2p)^{M+1}}{(2p-1)^2(M+1)(M+2)} - O\left(\frac{(2\bar{p})^{M+1}}{M+2}\right).
\end{split}
\end{equation}
Using the Taylor expansion of function $\log(x)$, we can calculate the ratio in Lemma~\ref{Ratio1} as
\begin{equation}
\log\frac{H(0,p)}{H(M,p)}=\log(M)+\log\frac{2p-1}{p}+O\left(\frac{1}{M}\right)
\end{equation}
which concludes Lemma~\ref{Ratio1} when $M\gg 1$.

Furthermore, we can solve $H(1,p)$ as
\begin{equation}
\begin{split}
H(1,p)&= \omega(p)\int_{\bar{p}}^{p}x^{M-1}(x-\bar{p})\mathrm{d}x-H(0,p)\\
&=\frac{(2\bar{p}+M^{-1})(2p)^{M}}{(2p-1)(M+1)} - O\left(\frac{(2p)^{M+1}}{M^2}\right)
\end{split}
\end{equation}
The value ratio between $m=0$ and $m=1$ then satisfies
\begin{equation}
\log \frac{H(1,p)}{H(0,p)}  = \log\frac{p}{\bar{p}+M^{-1}}+O\left(\frac{1}{M}\right).
\end{equation}
By Rolle's theorem, there exists a $c\in [m, m+1]$ satisfying
\begin{equation}
\log H(1,p) - \log H(0,p) = f'(c)
\end{equation}
where $f(m)=\log H(m,p)$. Meanwhile, Lemma~\ref{LogConvexity} ensures that $f''(m)\geq 0$ always holds. Thus, we can have
\begin{equation}
\log H(m+1,p) - \log H(m,p)\geq \log \frac{H(1,0)}{H(0,p)}
\end{equation}
which concludes the first case of Lemma~\ref{LowBound1}. Similarly, we compute the ratio between $m=M-1$ and $M$ as
\begin{equation}
\log \frac{H(M,p)}{H(M-1,p)}  = \log\frac{p}{\bar{p}+M^{-1}}+O\left(\frac{1}{M}\right).
\end{equation}
Meanwhile, Rolle's theorem and Lemma~\ref{LogConvexity} ensure that
\begin{equation}
\log H(m,p) - \log H(m-1,p)\leq \log \frac{H(M,0)}{H(M-1,p)}
\end{equation}
which concludes the second case of Lemma~\ref{LowBound1}.

Lastly, we focus on the upper bound described by Lemma~\ref{UpBound1}. According to the inequality of arithmetic and geometric means, $x(1-x)\leq 2^{-2}$ holds for any $x\in [0,1]$. Thus, when $2m\leq M$ (i.e. $n\geq m$), we can have
\begin{equation}
H(m,p)\leq 2^{-2m}\omega(p)\cdot \int_{\bar{p}}^{p}x^{n-m}(x-1+p)\mathrm{d}x
\end{equation}
where the equality only holds when $m=0$.
\begin{equation}
 \int_{\bar{p}}^{p}x^{n-m}(x-1+p)\mathrm{d}x=\frac{(2p-1)p^{\delta}}{\delta}+\frac{\Delta}{\delta(\delta+1)}
\end{equation}
where $\delta=n-m+1$ and $\Delta=\bar{p}^{\delta+1}-p^{\delta+1}<0$. Hence,
\begin{equation}
\log\frac{H(m,p)}{H(0,p)}\leq -2m[\log(2p)-\varepsilon(m)] + O\left(\frac{1}{M}\right)
\end{equation}
where $\varepsilon(m)=-(2m)^{-1}[\log(n-m+1)-\log(M+1)]$. Since $\log(x)$ is a concave function, we can know that
\begin{equation}
\varepsilon(m)\leq (M)^{-1}\log(M+1)=O\left(M^{-1}\right)
\end{equation}
which concludes the first case in Lemma~\ref{UpBound1}. Similarly, for $2m>M$ (i.e. $n<m$), we can have
\begin{equation}
\log\frac{H(m,p)}{H(M,p)}\leq -2n[\log(2p)-\hat{\varepsilon}(n)] + O\left(\frac{1}{M}\right)
\end{equation}
where $\hat{\varepsilon}(n)\leq O(M^{-1})$. Thereby, we can conclude the second case of Lemma~\ref{UpBound1}. Note that the case where $p<0.5$ can be derived by using Lemma~\ref{Symmetry}.
\end{proof}
\end{lemma}
For the case that $t=0$ and $M\gg 1$, using the same method as the above proof, we can derive the same lower and upper bounds as Lemmas~\ref{UpBound1} and \ref{LowBound1}. On the other hand, for $t=0$, Lemma~\ref{Ratio1} does not hold and we can have
\begin{lemma}
\label{Ratio0}
$H(m,p;M,0)=H(n,p;M,0)$
\begin{proof}
When $t=0$,
\begin{equation}
H(m,p)=2^{M+1}(2p-1)^{-1}\int_{\bar{p}}^{p}x^n(1-x)^m\mathrm{d}x.
\end{equation}
Then, substituting $x$ as $1-v$ concludes Lemma~\ref{Ratio0}.
\end{proof}
\end{lemma}

\section{Proof for Proposition 3}
Denote the prior distribution of $\bm{\theta}$ by $\pi$. Then,
\begin{align}
&P(\mathcal{L}|\bm{\alpha}, \bm{\beta})= {\prod}_{j=1}^{M}P_{\bm{\theta}}(\bm{x}_j) \int e^{[-M\cdot d_{KL}]} \mathrm{d}\pi(\hat{\bm{\theta}})\\
&d_{KL}=\frac{1}{M}\sum_{j=1}^{M}\log \frac{P_{\bm{\theta}}(\bm{x}_j)}{P_{\bm{\hat{\theta}}}(\bm{x}_j)}\rightarrow \mathrm{KL}[P_{\bm{\theta}}(\bm{x}),P_{\bm{\hat{\theta}}}(\bm{x})]
\end{align}
where $\bm{x}_j$ denotes the labels generated for task $j$. The KL divergence $\mathrm{KL}[\cdot, \cdot]$, which denotes the expectation of the log-ratio between two probability distributions, is a constant for the given $\bm{\theta}$ and $\hat{\bm{\theta}}$.
Thus, $\int e^{[-M\cdot d_{KL}]} \mathrm{d}\pi(\hat{\bm{\theta}})=C_{L}(M)$.
In addition, when $M\rightarrow \infty$, we can also have $\sum 1(\bm{x}_j=\bm{x})\rightarrow M \cdot P_{\bm{\theta}}(\bm{x})$, which concludes Proposition~3.

\section{Proof for Proposition 4}
Firstly, we introduce a set of variables to describe the real true labels and the collected labels.
Among the $n$ tasks of which the posterior true label is correct,
\begin{itemize}[noitemsep,topsep=0pt]
\item $x_0$ and $y_0$ denote the number of tasks of which the real true label is $1$ and $2$, respectively.
\item $x_i$ and $y_i$ denote the number of tasks of which worker $i$'s label is correct and wrong, respectively.
\end{itemize}
Also, among the remaining $m=M-n$ tasks, 
\begin{itemize}[noitemsep,topsep=0pt]
\item $w_0$ and $z_0$ denote the number of tasks of which the real true label is $1$ and $2$, respectively.
\item $w_i$ and $z_i$ denote the number of tasks of which worker $i$'s label is correct and wrong, respectively.
\end{itemize}
Thus, we can have $x_i+y_i=n$ and $w_i+z_i=m$. Besides, we use $\xi_i$ to denote the combination $(x_i,y_i,w_i, z_i)$.

To compute the expectation of $m/M$, we need to analyze the probability distribution of $m$. According to Equation~\ref{PostDist}, we can know that $P(m)$ satisfies
\begin{equation}
P(m) \approx \frac{C_{M}^{m}}{Z} \sum_{\xi_0,\ldots, \xi_N}\prod_{i=0}^{N}P(\xi_i|m) B(\hat{\bm{\beta}})\prod_{i=1}^{N}B(\hat{\bm{\alpha}}^{*}_{i})
\end{equation}
where $Z=C_pC_L{\prod}_{\bm{x}} [P_{\bm{\theta}}(\bm{x})]^{M\cdot P_{\bm{\theta}}(\bm{x})}$ is independent of $\xi_i$ and $m$.
Meanwhile, $\hat{\beta}_1=x_0+z_0+1$, $\hat{\beta}_2=y_0+w_0+1$, $\hat{\alpha}_{i1}^{*}=x_i+z_i+2$ and $\hat{\alpha}_{i2}^{*}=x_i+z_i+1$.
When the $m$ tasks of which the posterior true label is wrong are given, we can know that $x_i\sim \mathrm{Bin}(n, p_i)$ and $w_i\sim \mathrm{Bin}(m, p_i)$, where $\mathrm{Bin}(\cdot)$ denotes the binomial distribution.
In addition, $x_i$ and $y_i$ are independent of $w_i$, $z_i$ and $\xi_{k\neq i}$.
Also, $w_i$ and $z_i$ are independent of $x_i$ and $y_i$ and $\xi_{k\neq i}$.
Thus, we can further obtain $P(m)\approx \hat{Z}^{-1}\cdot C_{M}^{m}Y(m)$, where
\begin{equation}
\label{PDist}
\begin{split}
&Y(m) =e^{\log H(m,p_0;M,0)+\sum_{i=1}^{N}\log H(m,p_i;M,1)}\\
&H(m,p;M,t)={\sum}_{x=0}^{n}{\sum}_{w=0}^{m} 2^{M+1}C_{n}^{x}C_{m}^{w}\times\\
&\quad p^{x+w}(1-p)^{y+z}B(x+z+1+t,y+w+1)
\end{split}
\end{equation}
and $\hat{Z}=2^{-(N+1)(M+1)}Z$.
Considering $\sum_{m=1}^{M} P(m)=1$, we can know that $\hat{Z}\approx{\sum}_{m=1}^{M}C_{M}^{m}Y(m)$.

The biggest challenge of computing $P(m)$ exists in the analysis of function $H(m,p;M,t)$ which we put in the supplementary file because of the space limitation. 
Here, we directly use the obtained lower and upper bounds of the $H$ function (Lemmas~\ref{Su-LowBound1} and \ref{Su-UpBound1}) and can have
\begin{equation}
\left\{
\begin{array}{lc}
e^{C-{K}_l m}\lesssim Y(m) \lesssim e^{C-K_u m} & 2m\leq M\\
e^{C+\delta-{K}_l  n}\lesssim Y(m) \lesssim e^{C+\delta-K_u n} & 2m>M
\end{array}
\right.
\end{equation}
where $C=H(0,p_0;M,0)+\sum_{i=1}^{N}H(0,p_i;M,1)$ and
\begin{equation*}
\begin{split}
&K_l = {\sum}_{i=0}^{N}\log \hat{\lambda}_{i}\;,\; K_u =  2{\sum}_{i=0}^{N}\log \left(2\hat{p}_i\right)\\
&\delta = \Delta\cdot \log(M)+{\sum}_{i=1}^{N}(-1)^{1(p_i>0.5)}\phi(\hat{p}_i)\\
&\hat{\lambda}_i=\max\left\{\frac{p_i}{\bar{p}_i+\frac{1}{M}},\frac{\bar{p}_i}{p_i+\frac{1}{M}}\right\}
\;,\;\phi (p) =\log\frac{2p-1}{p}.
\end{split}
\end{equation*}
Besides, we set a convention that $\phi(p)=0$ when $p=0.5$. Thereby, the expectations of $m$ and $m^2$ satisfy
\begin{align}
\mathbb{E}[m] \lesssim \frac{\sum_{m=0}^{M}me^{-K_u m}+\sum_{m=0}^{M}me^{\delta-K_u n}}{\sum_{m=0}^{k}e^{-K_l m}+\sum_{m=k+1}^{M}e^{\delta-K_l n}}
\label{ExM1}\\
\mathbb{E}[m^2] \lesssim \frac{\sum_{m=0}^{M}m^2e^{-K_u m}+\sum_{m=0}^{M}m^2e^{\delta-K_u n}}{\sum_{m=0}^{k}e^{-K_l m}+\sum_{m=k+1}^{M}e^{\delta-K_l n}} \label{ExM2}
\end{align}
where $k=\lfloor M/2 \rfloor$.
By using Lemmas~\ref{Su-Sum2}, \ref{Su-Sum3}, \ref{Su-Sum5} and \ref{Su-Sum6}, we can know the upper bounds of the numerator in Equations~\ref{ExM1} and \ref{ExM2} are $M(\varepsilon+e^{\delta})(1+\varepsilon)^{M-1}$ and $[M^2\varepsilon^2+M\varepsilon+e^{\delta}(M^2+M\varepsilon)](1+\varepsilon)^{M-2}$, respectively, where $\varepsilon=e^{-K_u}$. On the other hand, by using Lemma~\ref{Su-Sum4}, we can obtain the lower bound of the denominator as $(1+e^{\delta})[1-e^{-c(\omega)M}](1+\omega)^{M}$, where $\omega=e^{-K_l}$ and $c(\omega)=0.5(1-\omega)^2(1+\omega)^{-2}$.
Considering $M\gg 1$, we can make the approximation that $e^{-c(\omega)M}\approx 0$ and $(1+e^\delta)\varepsilon/M\approx 0$. Besides, $(1+\omega)^{M}\geq 1$ holds because $\omega\geq 0$. In this case, Proposition~4 can be concluded by combining the upper bound of the numerator and the lower bound of the denominator.
%
%In this case, we can conclude the first inequality of Proposition~\ref{ConvBound} by combining these two bounds. Furthermore, by using Lemmas~\ref{Su-Sum5} and \ref{Su-Sum6} in the supplementary file, we can know that
%\begin{equation}
%\sum_{m=0}^{M}m^2(e^{-\overline{K}m}+e^{\delta-\overline{K}n})\approx (x^2+e^{\delta})(1+x)^{M-2}
%\end{equation}
%which concludes the second inequality of Proposition~\ref{ConvBound}.


\bibliographystyle{named}
\bibliography{ref}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
