\section{Game-Theoretic Analysis}
In this section, we present the game-theoretic analysis on our incentive mechanism. Our main results are as follows:
\begin{proposition}
\label{OSEqulibrium}
When $M\gg 1$ and $(2p_H)^{2(N-1)} \geq M$, in any time step $t$, reporting truthfully ($r^{t}_i = 0$) and exerting high efforts ($e^{t}_i=1$) is the payment-maximizing strategy for any worker $i$ if the other workers all follow this strategy. In other words, reporting truthfully and exerting high efforts is a Nash equilibrium for all workers in any time step.
\end{proposition}
\begin{proposition}
\label{RMNE}
Suppose the conditions in Proposition~\ref{OSEqulibrium} are satisfied. In our reinforcement learning algorithm, when $\tilde{Q}(s,a)$ approaches the real $Q(s,a)$ and
\begin{equation}
\label{Condition}
\eta \zeta \cdot  {\min}_{a,b\in\mathcal{A}}|a-b|> \frac{F(1)-F(1-\psi)}{1-\rho}
\end{equation}
always reporting truthfully ($r^{t}_i \equiv 0$) and exerting high efforts ($e^{t}_i\equiv 1$) is the payment-maximizing strategy for any worker $i$ in the long term if the other workers all follow this strategy.
In other words, always reporting truthfully and exerting high efforts is a Nash equilibrium for all workers.
\end{proposition}
The proof of Proposition~\ref{OSEqulibrium} relies on the convergence of our Bayesian inference algorithm, namely $\tilde{p}_i^t\rightarrow p_i^t$.
Proposition~\ref{RMNE} provides a novel idea about the game-theoretic analysis of the reinforcement learning algorithm.
More specifically, in the right-hand side Equation~\ref{Condition},
\begin{equation}
\psi =2(\tau_1\tau_2^{-1}+\tau_1^{-1}\tau_2)[4p_H(1-p_H)]^{\frac{N-1}{2}}
\end{equation}
is the upper bound of the label accuracy increment brought by a single worker.
Thus, the right-hand side Equation~\ref{Condition} indicates the upper bound of the long-term utility increment that a single worker can bring.
On the other hand, $\zeta= M(N-1)p_H$ and ${\min}_{a,b\in\mathcal{A}}|a-b|$ denotes the minimal gap between two available values of the scaling factor $a^t$.
Thus, the left-hand side of Equation~\ref{Condition} is actually the lower bound of the payment increment if our reinforcement adjustment algorithm increases the scaling factor.
Thereby, if Equation~\ref{Condition} is satisfied, a single worker will always be unable to cause our reinforcement adjustment algorithm to change $a^t$.
This property ensures always reporting truthfully and exerting high efforts to be a Nash equilibrium, and also prevents the clever manipulation that a worker scarifies short-term benefits for higher payments in the future.
In the remaining parts of this section, we will provide the details of our proof.
It is also worth noting that we prove over 10 lemmas as the foundation of our proof.
Due to the space limitation, we put them all in the supplementary file.

\subsection{Proof for Proposition~\ref{OSEqulibrium}}
After the workers report their labels, the payment in our incentive mechanism is only decided by $\tilde{p}^t_i$ which only depends on the labels in the current step.
Thus, in this subsection, we focus on analyzing our Bayesian inference algorithm and omit the superscript $t$ in all equations for the simplicity of notations.
From Equation~\ref{PostDist}, we can know the posterior distribution of the true labels satisfies
\begin{equation}
\label{postdist2}
P(\bm{L}|\mathcal{L}, \bm{\alpha}, \bm{\beta})=\frac{B(\hat{\bm{\beta}})\prod_{i=1}^{N}B(\hat{\bm{\alpha}}^{*}_{i})}{C_p\cdot P(\mathcal{L}|\bm{\alpha}, \bm{\beta})}
\end{equation}
where $C_p$ is the nomalization constant.
Denote the labels generated by $N$ workers for one task as vector $\bm{x}$.
Then, we can compute the distribution of $\bm{x}$ as
\begin{equation}
P_{\bm{\theta}}(\bm{x}) = {\sum}_{k=1}^{2}\tau_k{\prod}_{i=1}^{N}p_i^{1(x_i=k)}(1-p_i)^{1(x_i=3-k)}
\end{equation}
where $\bm{\theta}=[\tau_1, p_1,\ldots,p_N]$ denotes all the parameters.
For the denominator in Equation~\ref{postdist2}, we can have
\begin{proposition}
\label{Denominator}
When $M\rightarrow \infty$, 
\begin{equation}
P(\mathcal{L}|\bm{\alpha}, \bm{\beta})\rightarrow C_{L}(M) \cdot {\prod}_{\bm{x}} [P_{\bm{\theta}}(\bm{x})]^{M\cdot P_{\bm{\theta}}(\bm{x})}
\end{equation}
where $C_{L}(M)$ denotes a constant that depends on $M$.
\begin{proof}
Denote the prior distribution of $\bm{\theta}$ by $\pi$. Then,
\begin{align}
&P(\mathcal{L}|\bm{\alpha}, \bm{\beta})= {\prod}_{j=1}^{M}P_{\bm{\theta}}(\bm{x}_j) \int e^{[-M\cdot d_{KL}]} \mathrm{d}\pi(\hat{\bm{\theta}})\\
&d_{KL}=\frac{1}{M}\sum_{j=1}^{M}\log \frac{P_{\bm{\theta}}(\bm{x}_j)}{P_{\bm{\hat{\theta}}}(\bm{x}_j)}\rightarrow \mathrm{KL}[P_{\bm{\theta}}(\bm{x}),P_{\bm{\hat{\theta}}}(\bm{x})]
\end{align}
where $\bm{x}_j$ denotes the labels generated for task $j$. The KL divergence $\mathrm{KL}[\cdot, \cdot]$, which denotes the expectation of the log-ratio between two probability distributions, is a constant for the given $\bm{\theta}$ and $\hat{\bm{\theta}}$.
Thus, $\int e^{[-M\cdot d_{KL}]} \mathrm{d}\pi(\hat{\bm{\theta}})=C_{L}(M)$.
In addition, when $M\rightarrow \infty$, we can also have $\sum 1(\bm{x}_j=\bm{x})\rightarrow M \cdot P_{\bm{\theta}}(\bm{x})$, which concludes Proposition~\ref{Denominator}.
\end{proof}
\end{proposition}

Then, we move our focus to the posterior true label vector $\bm{L}$ generated by $P(\bm{L}|\mathcal{L},\bm{\alpha}, \bm{\beta})$.
We introduce $n$ and $m$ to denote the number of tasks of which the posterior true label is correct and wrong, respectively.
Besides, for the simplicity of notations, we employ the convention that $\bar{p}=1-p$, $\hat{p}=\max \{p, \bar{p}\}$ and $p_0=\tau_1$. Hence, we can have
\begin{proposition}
\label{ConvBound}
When $M\gg 1$,
\begin{align}
\mathbb{E}[m/M]&\lesssim (1+e^{\delta})^{-1}(\varepsilon+e^{\delta})(1+\varepsilon)^{M-1}\\
\mathbb{E}[m/M]^2&\lesssim (1+e^{\delta})^{-1}(\varepsilon^2+e^{\delta})(1+\varepsilon)^{M-2}
\end{align}
where $\varepsilon^{-1}=\prod_{i=0}^{N}(2\hat{p}_i)^{2}$, $\delta=O[\Delta\cdot \log(M)]$ and 
$$\Delta={\sum}_{i=1}^N[1(p_i<0.5)-1(p_i>0.5)].$$
%Suppose the number of workers whose real score is lower and higher than $0.5$ are $N_{>0.5}$ and $N_{<0.5}$, respectively.
%Then, when $M\gg 1$, the expectation of the number of wrong labels satisfies
%\begin{equation*}
%\begin{split}
%\mathbb{E}_{\mathcal{L},\bm{L}}\left[\frac{m}{M}\right]&\lesssim \frac{(x+e^{\delta})(1+x)^{M-1}}{(1+e^{\delta})[1-e^{-c(y)M}](1+y)^{M}}\\
%\mathbb{E}_{\mathcal{L},\bm{L}}\left[\frac{m}{M}\right]^2&\lesssim \frac{(x^2+e^{\delta})(1+x)^{M-2}}{(1+e^{\delta})[1-e^{-c(y)M}](1+y)^{M}}
%\end{split}
%\end{equation*}
%where $x=\left(4^{N+1}\prod_{i=0}^{N}\hat{p}^{2}_i\right)^{-1}$, $y=\left(\prod_{i=0}^{N}\hat{\lambda}_{i}\right)^{-1}$ and
%\begin{equation*}
%\begin{split}
%&\quad p_0=\tau_1\;,\;\delta = [N_{<0.5}-N_{>0.5}]\log(M)+\phi(\bm{p})\\
%&\phi(\bm{p})={\sum}_{i=1}^{N}(-1)^{1(p_i>0.5)}\varepsilon(\hat{p}_i)\;,\; c(y)=\frac{(1-y)^2}{2(1+y)^{2}}\\
%&\hat{p}_i=\max\{p_i, 1-p_i\}, \hat{\lambda}_i=\max\left\{\frac{p_i}{\bar{p}_i+\frac{1}{M}},\frac{\bar{p}_i}{p_i+\frac{1}{M}}\right\}.
%\end{split}
%\end{equation*}
\begin{proof}
Firstly, we introduce a set of variables to describe the real true labels and the collected labels.
Among the $n$ tasks of which the posterior true label is correct,
\begin{itemize}[noitemsep,topsep=0pt]
\item $x_0$ and $y_0$ denote the number of tasks of which the real true label is $1$ and $2$, respectively.
\item $x_i$ and $y_i$ denote the number of tasks of which worker $i$'s label is correct and wrong, respectively.
\end{itemize}
Also, among the remaining $m=M-n$ tasks, 
\begin{itemize}[noitemsep,topsep=0pt]
\item $w_0$ and $z_0$ denote the number of tasks of which the real true label is $1$ and $2$, respectively.
\item $w_i$ and $z_i$ denote the number of tasks of which worker $i$'s label is correct and wrong, respectively.
\end{itemize}
Thus, we can have $x_i+y_i=n$ and $w_i+z_i=m$. Besides, we use $\xi_i$ to denote the combination $(x_i,y_i,w_i, z_i)$.

To compute the expectation of $m/M$, we need to analyze the probability distribution of $m$. According to Equation~\ref{PostDist}, we can know that $P(m)$ satisfies
\begin{equation}
P(m) \approx \frac{C_{M}^{m}}{Z} \sum_{\xi_0,\ldots, \xi_N}\prod_{i=0}^{N}P(\xi_i|m) B(\hat{\bm{\beta}})\prod_{i=1}^{N}B(\hat{\bm{\alpha}}^{*}_{i})
\end{equation}
where $Z=C_pC_L{\prod}_{\bm{x}} [P_{\bm{\theta}}(\bm{x})]^{M\cdot P_{\bm{\theta}}(\bm{x})}$ is independent of $\xi_i$ and $m$.
Meanwhile, $\hat{\beta}_1=x_0+z_0+1$, $\hat{\beta}_2=y_0+w_0+1$, $\hat{\alpha}_{i1}^{*}=x_i+z_i+2$ and $\hat{\alpha}_{i2}^{*}=x_i+z_i+1$.
When the $m$ tasks of which the posterior true label is wrong are given, we can know that $x_i\sim \mathrm{Bin}(n, p_i)$ and $w_i\sim \mathrm{Bin}(m, p_i)$, where $\mathrm{Bin}(\cdot)$ denotes the binomial distribution.
In addition, $x_i$ and $y_i$ are independent of $w_i$, $z_i$ and $\xi_{k\neq i}$.
Also, $w_i$ and $z_i$ are independent of $x_i$ and $y_i$ and $\xi_{k\neq i}$.
Thus, we can further obtain $P(m)\approx 2^{-(N+1)(M+1)}Z^{-1}\cdot C_{M}^{m}Y(m)$, where
\begin{equation}
\label{PDist}
\begin{split}
&Y(m) =e^{\log H(m,p_0;M,0)+\sum_{i=1}^{N}\log H(m,p_i;M,1)}\\
&H(m,p;M,t)={\sum}_{x=0}^{n}{\sum}_{w=0}^{m} 2^{M+1}C_{n}^{x}C_{m}^{w}\times\\
&\quad p^{x+w}(1-p)^{y+z}B(x+z+1+t,y+w+1).
\end{split}
\end{equation}
Besides, considering $\sum_{m=1}^{M} P(m)=1$, we can know that
\begin{equation}
2^{-(N+1)(M+1)}\cdot Z\approx{\sum}_{m=1}^{M}C_{M}^{m}Y(m).
\end{equation}

The biggest challenge of computing $P(m)$ exists in the analysis of function $H(m,p;M,t)$ which we put in the supplementary file because of the space limitation. 
Here, we directly use the obtained lower and upper bounds of the $H$ function (Lemmas~\ref{Su-LowBound1} and \ref{Su-UpBound1}) and can have
\begin{equation}
\left\{
\begin{array}{lc}
e^{C-{K}_l m}\lesssim Y(m) \lesssim e^{C-K_u m} & 2m\leq M\\
e^{C+\delta-{K}_l  n}\lesssim Y(m) \lesssim e^{C+\delta-K_u n} & 2m>M
\end{array}
\right.
\end{equation}
where $C=H(0,p_0;M,0)+\sum_{i=1}^{N}H(0,p_i;M,1)$ and
\begin{equation*}
\begin{split}
&K_l = {\sum}_{i=0}^{N}\log \hat{\lambda}_{i}\;,\; K_u =  2{\sum}_{i=0}^{N}\log \left(2\hat{p}_i\right)\\
&\delta = \Delta\cdot \log(M)+{\sum}_{i=1}^{N}(-1)^{1(p_i>0.5)}\phi(\hat{p}_i)\\
&\hat{\lambda}_i=\max\left\{\frac{p_i}{\bar{p}_i+\frac{1}{M}},\frac{\bar{p}_i}{p_i+\frac{1}{M}}\right\}
\;,\;\phi (p) =\log\frac{2p-1}{p}.
\end{split}
\end{equation*}
Besides, we set a convention that $\phi(p)=0$ when $p=0.5$. Thereby, the expectations of $m$ and $m^2$ satisfy
\begin{align}
\mathbb{E}[m] \lesssim \frac{\sum_{m=0}^{M}me^{-K_u m}+\sum_{m=0}^{M}me^{\delta-K_u n}}{\sum_{m=0}^{k}e^{-K_l m}+\sum_{m=k+1}^{M}e^{\delta-K_l n}}
\label{ExM1}\\
\mathbb{E}[m^2] \lesssim \frac{\sum_{m=0}^{M}m^2e^{-K_u m}+\sum_{m=0}^{M}m^2e^{\delta-K_u n}}{\sum_{m=0}^{k}e^{-K_l m}+\sum_{m=k+1}^{M}e^{\delta-K_l n}} \label{ExM2}
\end{align}
where $k=\lfloor M/2 \rfloor$.
By using Lemmas~\ref{Su-Sum2}, \ref{Su-Sum3}, \ref{Su-Sum5} and \ref{Su-Sum6}, we can know the upper bounds of the numerator in Equations~\ref{ExM1} and \ref{ExM2} are $M(\varepsilon+e^{\delta})(1+\varepsilon)^{M-1}$ and $[M^2\varepsilon^2+M\varepsilon+e^{\delta}(M^2+M\varepsilon)](1+\varepsilon)^{M-2}$, respectively, where $\varepsilon=e^{-K_u}$. On the other hand, by using Lemma~\ref{Su-Sum4}, we can obtain the lower bound of the denominator as $(1+e^{\delta})[1-e^{-c(\omega)M}](1+\omega)^{M}$, where $\omega=e^{-K_l}$ and $c(\omega)=0.5(1-\omega)^2(1+\omega)^{-2}$.
Considering $M\gg 1$, we can make the approximation that $e^{-c(\omega)M}\approx 0$ and $(1+e^\delta)\varepsilon/M\approx 0$. Besides, $(1+\omega)^{M}\geq 1$ holds because $\omega\geq 0$. In this case, Proposition~\ref{ConvBound} can be concluded by combining the upper bound of the numerator and the lower bound of the denominator.
%
%In this case, we can conclude the first inequality of Proposition~\ref{ConvBound} by combining these two bounds. Furthermore, by using Lemmas~\ref{Su-Sum5} and \ref{Su-Sum6} in the supplementary file, we can know that
%\begin{equation}
%\sum_{m=0}^{M}m^2(e^{-\overline{K}m}+e^{\delta-\overline{K}n})\approx (x^2+e^{\delta})(1+x)^{M-2}
%\end{equation}
%which concludes the second inequality of Proposition~\ref{ConvBound}.
\end{proof}
\end{proposition}
Lastly, focusing on worker $i$, we calculate the difference between the estimated PoBC $\tilde{p}_i$ and the real PoBC $p_i$ when the other workers all exert high efforts and report truthfully.
When $M\gg 1$, according to Equation~\ref{p_infer}, we can know that $\tilde{p}_i\approx \mathbb{E}_{\bm{L}}(x_i+z_i)/M$, where $\mathbb{E}_{\bm{L}}$ denotes the expectation based on the posterior distribution $P(\bm{L}|\mathcal{L})$. Meanwhile, in the proof of Proposition~\ref{ConvBound}, according to the law of large numbers, $p_i\approx (x_i+w_i)/M$. Thus, we can have
\begin{equation}
|\tilde{p}_i-p_i|\approx \mathbb{E}_{\bm{L}}|w_i-z_i|/M\leq \mathbb{E}_{\bm{L}}\left[m/M\right].
\end{equation}
If workers except for worker $i$ all report truthfully and exert high efforts, then $\Delta \leq -1$ in Proposition~\ref{ConvBound} because we require $N\geq 3$ in Section~\ref{PF}.
Considering $M\gg 1$, we can make the approximation that $e^{\delta}\approx 0$.
In addition, considering $2\hat{p}_i\geq 1$,we can have $\varepsilon^{-1}\geq (2p_H)^{2(N-1)}$.
When $(2p_H)^{2(N-1)} \geq M$, $\varepsilon\leq M^{-1}$.
Thus, the upper bound in Proposition~\ref{ConvBound} can be further calculated as
\begin{equation}
\mathbb{E}\left[\frac{m}{M}\right]\lesssim \frac{C_{1}}{M\cdot C_2}\;\;, \;\;\mathbb{E}\left[\frac{m}{M}\right]^2\lesssim \frac{C_{1}}{M^2\cdot C_2^2}
\end{equation}
where $C_{1}=(1+M^{-1})^{M}\approx e$ and $C_{2}=1+M^{-1}\approx 1$.
Then, $m/M\approx 0$ because $\mathbb{E}[m/M]\approx 0$ and $\mathrm{Var}[m/M]=\mathbb{E}[m/M]^2-(\mathbb{E}[m/M])^2 \approx 0$.
In this case, $\tilde{p}_i\approx p_i$.
Thereby, worker $i$ can only get the maximal payment when reporting truthfully and exerting high efforts, namely, when $p_i=p_H$, which concludes Proposition~\ref{OSEqulibrium}.

%Thus, $\mathbb{E}_{\mathcal{L},\bm{L}}\left[m/M\right]\lesssim x(1+x)^{M-1}$. Considering $2\hat{p}\geq 1$, we can know that $x\geq (2p_H)^{2(N-1)}$. Then, when $M\rightarrow \infty$ and $(2p_H)^{2(N-1)}>M$, we can have $(1+x)^{M-1}\rightarrow e$ and $x\rightarrow 0$. In this case, $\mathbb{E}_{\mathcal{L}}|\tilde{p}_i-p_i|\rightarrow 0$.
%Besides, we know that $p_i$ reaches the maximum only when worker $i$ report truthfully and exert high efforts.
%Thereby, in our mechanism, workers can only maximize their rewards by reporting truthfully and exerting high efforts, which concludes Proposition~\ref{OSEqulibrium}.
%for the simplicity of notation, we write $\mathbb{E}_{\mathcal{L},\bm{L}}[\frac{m}{M}]$ and $\mathbb{E}_{\mathcal{L},\bm{L}}[\frac{m}{M}]^2$ as $u$ and $v$, respectively.
%Recalling our mechanism defined in Definition~\ref{MechDef}, to analyze workers' expected rewards, we need to calculate the expectation of $\hat{p}_i$, $\hat{p}^2_0$ and $(1-\hat{p}_0)^2$ at first.
%According to Equation~\ref{p_infer}, we know $\hat{p}_i\approx \mathbb{E}_{\bm{L}}(x_i+z_i)/M$. Thus, the expectation of $\hat{p}_i$ satisfies
%\begin{equation}
%\mathbb{E}_{\mathcal{L}}\hat{p}_i\approx{\sum}_{m,x_i,z_i}\frac{x_i+z_i}{M}P(x_i|n)P(z_i|m)P(m)
%\end{equation}
%For a given $m$, $x_i\sim\mathrm{Bin}(n, p_i)$ and $z_i\sim\mathrm{Bin}(m, 1-p_i)$.
%Thus, we can calculate the expectation as $\mathbb{E}_{\mathcal{L}}\hat{p}_i = p_i\left(1-u\right)+(1-p_i)u$.
%Meanwhile, $\hat{p}^2_0$ satisfies
%\begin{equation}
%\mathbb{E}_{\mathcal{L}}\hat{p}^2_0\approx{\sum}_{m=0}^{M}\frac{\mathbb{E}x^2_0+2\mathbb{E}x_0\mathbb{E}z_0+\mathbb{E}z^2_0}{M^2}P(m)
%\end{equation}
%where the expectation of the square should satisfy $\mathbb{E}x^2_0=(\mathbb{E}x_0)^2+\mathrm{Var}(x_0)$. Hence, $\mathbb{E}_{\mathcal{L}}\hat{p}^2_0\approx p_0^2+2p_0^2(v-u)$. Similarly, we can get $\mathbb{E}_{\mathcal{L}}(1-\hat{p}_0)^2\approx \bar{p}_0^2+2\bar{p}_0^2(v-u)$. Thereby, the expectation of worker $i$'s reward can be calculated as
%\begin{equation}
%\mathbb{E}_{\mathcal{L}} [r_i] = p_i - c_0 +(1-2p_i+2c_0)u-2c_0v
%\end{equation}
%where $c_0=p_0^2+\bar{p}_0^2\geq 0.5$ and $1-2p_i+2c_0\geq 0$.
%Thereby, if $\mathbb{E}_{\mathcal{L},\bm{L}}\left[m/M\right]\leq 0.5$ holds for any $p_i$, $\mathbb{E}_{\mathcal{L}}\hat{p}_i$ will be a monotonically increasing function of $p_i$---i.e. worker $i$ will be able to get the maximal reward from our mechanism by reporting truthfully and exerting high efforts.
%
%For worker $i$, suppose all other workers report truthfully and exert high efforts. Then, in Proposition~\ref{ConvBound}, $N_{<0.5}-N_{>0.5}\leq 2-N\leq -1$. Since $M\gg 1$, $e^{\delta}\approx 0$ and $e^{-M}\approx0$. Besides, if the upper bounds in Proposition~\ref{ConvBound} become larger than $1.0$, they will lose the function to bound the number of error labels. Then, we need ensure $y\leq x \ll 1$, and thus
%\begin{equation}
%u\lesssim \hat{u} = \xi^{-1}(1+\xi^{-1})^{M-1}\;,\; v\approx 0.
%\end{equation} 
%Hence, if worker $i$ report truthfully and exert high efforts, the lower bound of $r_i$ is $p_H-c_0$.
%If worker $i$ report truthfully and exert no efforts, the upper bound of $r_i$ is $0.5-c_0+2\hat{u}$.
%If worker $i$ report falsely and exert high efforts, the upper bound of $r_i$ is $\bar{p}_H-c_0+(1+2\bar{p}_H)\hat{u}$.
%To ensure reporting truthfully and exerting high efforts is a Nash equilibrium for all workers, worker $i$ should be able to get the maximal reward by following this equilibrium strategy.
%Considering $p_H>0.5$, we can know the condition for ensuring the Nash equilibrium is $0.5-c_0+2\hat{u}\leq p_H-c_0$, which concludes Proposition~\ref{OSEqulibrium} by eliminating $c_0$ from the equation.
%Lastly, we provide the conditions for small $\mathbb{E}_{\mathcal{L},\bm{L}}\left[m/M\right]$.
%\begin{proposition}
%\label{SmallError}
%$\mathbb{E}_{\mathcal{L},\bm{L}}\left[m/M\right]\rightarrow 0$ when
%\begin{equation}
%M\rightarrow \infty\;,\;N_{<0.5}<N_{>0.5}\;,\;(2p_H)^{2(N-1)}>M
%\end{equation}
%\vspace{-3mm}
%\begin{itemize}
%\item $M\rightarrow \infty$;
%\item $N_{<0.5}<N_{>0.5}$;
%
%\end{itemize}
%\end{proposition}
%
%We present the threshold values of $N$ corresponding to Proposition~\ref{OSEqulibrium} in Table~\ref{icr-table}. They are obtained by solving the equation $2\xi^{-1}(1+\xi^{-1})^{M-1}=1-p_H$ .
%From the table, we can know that, to prevent the errors of our Bayesian inference algorithm from causing incorrect rewards, if workers' capability is low, we should increase the number of workers to ensure the quality of collected labels.
%If the number of tasks increases, we should also slightly increase the number of workers to ensure the high accuracy of $\hat{p}_i$.
%\begin{table}[t]
%\caption{\#Workers needed for ensuring the Nash equilibrium}
%\label{icr-table}
%%\vskip 0.05in
%\begin{center}
%\begin{small}
%\begin{sc}
%\begin{tabular}{lccccr}
%\toprule
%\#Tasks & 100 & 200 & 1000 & 10000\\
%\midrule
%$p_H=0.7$    & 7.00 & 7.69 & 9.45 & 12.22 \\
%$p_H=0.8$  & 5.14 & 5.66 & 6.95 & 8.97 \\
%$p_H=0.9$     & 4.23 & 4.66 & 5.71 & 7.33 \\
%\bottomrule
%\end{tabular}
%\end{sc}
%\end{small}
%\end{center}
%\vskip -0.1in
%\end{table}
%
%Except for the Nash equilibrium defined in Proposition~\ref{OSEqulibrium}, our mechanism also has other equilibria. We summarize them here:

\subsection{Proof for Proposition~\ref{RMNE}}
{\color{red}
\begin{equation}
\label{Qfun}
Q(s_t, a^t)= {\sum}_{i=0}^{\infty} \rho^{i} u_{t+i}.
\end{equation}
\begin{equation}
Q(s,a) = \bm{k}_t(s,a)^{T} (\bm{K}_t+\sigma^2 \bm{I}_t)^{-1} H_t^{-1} \bm{u}_t
\end{equation}
where $\bm{k}_t(s,a) = [k(x, x_0), \ldots, k(x, x_t)]^{T}$, $\bm{K}_t=[\bm{k}_t(s_0,a_0),\ldots,\bm{k}_t(s_t,a_t)]$ and $x=(s,a)$. Besides,
\begin{equation}
H_t = \left[
\begin{array}{ccccc}
1 & \rho & \rho^2 &\ldots & \rho^{t+1}\\
0 & 1 & \rho & \ldots & \rho^{t}\\
0 & 0 & 1 & \ldots & \rho^{t-1} \\
\vdots & \vdots & \vdots & \ddots& \vdots\\
0 & 0 & 0 & \ldots & 1
\end{array}
\right]
\end{equation}
}\\

To prove Proposition~\ref{RMNE}, we need to analyze worker $i$'s effects on our reinforcement learning algorithm.
If worker $i$ wishes to get higher payments in the long term, he/she must push our reinforcement learning algorithm to at least increase the scaling factor from $a$ to $b>a$ at a certian state $s$.
In the $\epsilon$-greedy strategy used by our reinforcement learning algorithm, the random selection part is independent of worker $i$.
Thus, worker $i$ must mislead the greedy part by letting $\tilde{Q}(s,a)\leq \tilde{Q}(s,b)$.
In this proof, we will show that, under the condition defined in Equation~\ref{Condition}, there does not exist $b\in \mathcal{A}$ that can achieve this objective.
In other words, our reinforcement learning algorithm will never increase the scaling factor to please a single worker.
On the other hand, in any time step $t$, worker $i$ will loss some money if $p_i^t<p_H$.
Thereby, the payment-maximizing strategy for worker $i$ is to report truthfully and exert high efforts in all time steps, which concludes Proposition~\ref{RMNE}.

Since Proposition~\ref{RMNE} requires $\tilde{Q}(s,a)\approx Q(s,a)$ as one of the conditions, we now focus on proving that $Q(s,a)- Q(s,b) >0$ always holds.
Suppose all workers except for worker $i$ report truthfully and exert high efforts in all time steps.
According to Equations~\ref{utility} and \ref{Qfun}, we can have $Q(s,a)- Q(s,b) \geq  X(a)-X(b) + Y$, where 
\begin{equation}
X(a)={\sum}_{k=0}^{\infty}\rho^{k}\cdot \mathbb{E}F(\tilde{A}^{k+t}|s_t= s^{*}, a^t=a)
\end{equation}
denotes the expected long-term utility that we get from the labels.
$Y= \eta M(N-1)p_H (b-a)>0$ denotes the payment increment for workers except worker $i$.
To attract our reinforcement learning algorithm to increase the scaling factor, worker $i$ must increase $p_i^t$ when $a^t$ is increased from $a$ to $b$.
Otherwise, we will get less accurate labels with higher payments, which is impossible for the greedy strategy used in our reinforcement learning algorithm.
In this case, the payment for worker $i$ will also increase.
However, we do not know $p_i$. Thus, we regard the payment increment as $0$ when deriving the lower bound of $Q(s,a)- Q(s,b)$.

% \frac{F_u-F_l}{1-\rho} - \eta \zeta (b-a)
%
%When $t\gg 1$, we can neglect the effects of the end-point and make the approximation that $H_t^{-1} \bm{u}_t\approx \bm{U}_t$, where the vector of the long-term cumulative utility $\bm{U}_t=[U(0), U(1),\ldots, U(t)]$.
%Due to the random selection part of the $\epsilon$-greedy strategy, there should be many state-action combinations in the neighborhood of $(s^*,a)$ and $(s^*,b)$ that have been tested by our reinforcement learning algorithm.
%These data will play the dominant role in the Gaussian process regression that
%\begin{equation}
%Q(s^*,a) = \bm{k}_t(s^*,a)^{T} (\bm{K}_t+\sigma^2 \bm{I}_t)^{-1} \bm{U}_t.
%\end{equation}
%Thus, if we can prove that $U(t,a^t=a)>U(t,a^t=b)$ holds for any time step and $b\in \mathcal{A}$, we can concldue that $Q(s^*,a)\leq Q(s^*,b)$ is impossible.
%
%
%the long-term utility $U(t)$, which consists of the utility got from the labels and lost in the payments.

Here, to bound $X(a)-X(b)$, we analyze the effects of worker $i$ on the estimated accuracy $\tilde{A}$.
Since our analysis is satisfied in all time steps, we omit the time step $t$ for the simplicity of notations.
From Equation~\ref{vot}, we can know that, when $M\gg 1$, the estimated accuracy $\tilde{A}$ satisfies
\begin{equation}
\label{accP}
\tilde{A} \approx 1-\mathbb{E}g(\tilde{\sigma}_j)\;\;,\;\; g(\tilde{\sigma}_j)=1/(1+e^{|\tilde{\sigma}_j|}).
\end{equation}
From the proof of Proposition~\ref{OSEqulibrium}, we can know that $\tilde{p}^{t}_i \approx p^{t}_i$.
In this case, according to Equation~\ref{ProbRatio}, we can have
\begin{equation}
\label{ProbRatioApp}
\tilde{\sigma}_j(p_i)\approx \log\left(\frac{\tau_{1}}{\tau_{2}}\lambda_i^{\delta_{ij1}-\delta_{ij2}}{\prod}_{k\neq i}\lambda_H^{\delta_{kj1}-\delta_{kj2}}\right).
\end{equation}
where $\lambda_i=p_i/(1-p_i)$ and $\lambda_H=p_H/(1-p_H)$.
%In this proof, if the superscripts of variables in an equation are not specified, the equation should hold for any time step $t$. Suppose all workers except worker $i$ report truthfully and exert high efforts at all time steps.
%Then, at step $t$, according to Proposition~\ref{ConvBound}, under the condition of Proposition~\ref{OSEqulibrium}, 
%both $\mathbb{E}[m/M]$ and $\mathbb{E}[m/M]^2$ approaches $0$.
%In this case, $\tilde{p}_i\rightarrow p_i$. Thus, the log-ratio, which is used for computing the posterior accuracy in Equation~\ref{vot} satisfies
%\begin{equation}
%\label{ProbRatioApp}
%\tilde{\sigma}_j(p_i)\approx \log\left(\frac{\tau_{1}}{\tau_{2}}\lambda_i^{\delta_{ij1}-\delta_{ij2}}{\prod}_{k\neq i}\lambda_H^{\delta_{kj1}-\delta_{kj2}}\right).
%\end{equation}

Considering the case that worker $i$ exert low efforts and reports randomly, namely $p_i=0.5$, we can eliminate $\lambda_i$ from Equation~\ref{ProbRatioApp} because $\lambda_i=1$.
Furthermore, according to Lemma~\ref{Su-Concave2} in the supplementary file, we can know that 
$g(\tilde{\sigma}_j)< e^{\tilde{\sigma}_j}$ and $g(\tilde{\sigma}_j)< e^{-\tilde{\sigma}_j}$ both hold.
Thus, we build a more tight upper bound of $g(\tilde{\sigma}_j)$ by dividing all the combinations of $\delta_{kj1}$ and $\delta_{kj2}$ in Equation~\ref{ProbRatioApp} into two sets and using the smaller one of $e^{\tilde{\sigma}_j}$ and $e^{-\tilde{\sigma}_j}$ in each set.
By using this method, if the true label is $1$, we can have $\mathbb{E}_{[L(j)=1]}g(\tilde{\sigma}_j)< q_1+q_2$, where
\begin{equation*}
\begin{split}
&q_1 = \frac{\tau_2}{\tau_1}{\sum}_{n=K+1}^{N-1}C_{N-1}^{n} (\frac{1}{\lambda_H})^{n-m}p_H^n(1-p_H)^m\\
&q_2 = \frac{\tau_1}{\tau_2}{\sum}_{n=0}^{K}C_{N-1}^{n} {\lambda_H}^{n-m}p_H^n(1-p_H)^m\\
&n={\sum}_{k\neq i}\delta_{kj1}\;,\;m= {\sum}_{k\neq i}\delta_{kj2}\;,\;K=\lfloor (N-1)/2 \rfloor.
\end{split}
\end{equation*}
Here, we use $e^{-\tilde{\sigma}_j}$ and $e^{\tilde{\sigma}_j}$ as the upper bound of $g(\tilde{\sigma}_j)$ when $n\in (K, N-1]$ and $n\in [0, K]$, respectively. By using Lemma~\ref{Su-Concave3} in the supplementary file, we can thus get
\begin{equation}
\begin{split}
\mathbb{E}_{[L(j)=1]}g(\tilde{\sigma}_j) < c_{\tau}[4p_H(1-p_H)]^{\frac{N-1}{2}}.
\end{split}
\end{equation}
where $c_{\tau}=\tau_1\tau_2^{-1}+\tau_1^{-1}\tau_2$. Similarly,
\begin{equation}
\begin{split}
\mathbb{E}_{[L(j)=2]}g(\tilde{\sigma}_j) < c_{\tau}[4p_H(1-p_H)]^{\frac{N-1}{2}}.
\end{split}
\end{equation}
Thereby, $\tilde{A}>1-2c_{\tau}[4p_H(1-p_H)]^{\frac{N-1}{2}}=1-\psi$.


We then consider another case where worker $i$ exerts high efforts but reports falsely, namely $p_i=1-p_H$. In this case, we can rewrite Equation~\ref{ProbRatioApp} as
\begin{equation}
\tilde{\sigma}_j(1-p_H)\approx \log\left(\frac{\tau_{1}}{\tau_{2}}\lambda_H^{x-y}{\prod}_{k\neq i}\lambda_H^{\delta_{kj1}-\delta_{kj2}}\right).
\end{equation}
where $x=\delta_{ij2}$ and $y=\delta_{ij1}$. Since $p_i=1-p_H$, $x$ and $y$ actually has the same distribution as $\delta_{kj1}$ and $\delta_{kj2}$. Thus, the distribution of $\tilde{\sigma}_j(1-p_H)$ is actually the same as $\tilde{\sigma}_j(p_H)$.
In other words, since Proposition~\ref{OSEqulibrium} ensures $p_i$ to be accurately estimated, our Bayesian inference algorithm uses the information provided by worker $i$ via flipping the label when $p_i<0.5$.
Thus, $p_i=0.5$ actually lowers $\tilde{A}$ to the utmost because worker $i$ provides no information about the true label in this case.
Thus, $\tilde{A}\geq 1-\psi$ always holds.
On the other hand, $\tilde{A}\leq 1.0$ also always holds.
Considering $F(\cdot)$ is a non-decreasing monotonic function, we can get $X(a)\geq (1-\rho)^{-1}F(1-\psi)$ while $X(b) \leq (1-\rho)^{-1}F(1)$.
Thereby, when Equation~\ref{Condition} is satisfied, $X(a)-X(b)+Y>0$ always holds, which concludes Proposition~\ref{RMNE}.