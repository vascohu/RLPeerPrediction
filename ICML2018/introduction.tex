\section{Introduction}
The ability to quickly collect large scale and high quality labeled datasets is crucial for Machine Learning (ML), and more generally for Artificial Intelligence. Among all proposed solutions, one of the most promising ones is crowdsourcing \cite{slivkins2014online,difallah2015dynamics,simpson2015language}. The idea is neat - instead of using a centralized amount of efforts, the to-be-labeled tasks are disseminated to a decentralized crowd of workers to parallelize the collection procedure, leveraging the power of human computation. Nonetheless, it has been noted that crowdsourced data often suffers from quality issues, due to its unique feature of no monitoring and no ground-truth verification of workers' contributed data. %cite{waggoner2014output}. 

This quality control challenge has been attempted by two relatively disconnected research communities. From the more ML side, quite a few inference techniques have been developed for inferring true labels from crowdsourced and potentially noisy labels \cite{dawid1979maximum,raykar2010learning,liu2012variational,chen2015statistical,zheng2017truth}. These solutions often work as a one-shot, post-processing procedure facing a static set of workers, whose labeling accuracy is fixed and \emph{informative}. Despite its nice theoretical contribution and empirical success, the above methods ignored the effects of \emph{incentives} when dealing with human inputs. It has been observed both in theory and practice that {\color{red} xxx\cite{liu2017sequential}}, without appropriate incentive, selfish and rational workers can easily chose to contribute low quality, uninformative, or even malicious data. Existing inference algorithms are very vulnerable in these cases - either much more redundant crowdsourced labels will need to be collected (low quality inputs), or the methods will simply fail to work (the case with uninformative and malicious inputs). 

From the less ML side, the above quality control question has been studied in the context of \emph{incentive mechanism design}. In particular, a family of mechanisms, jointly called \emph{peer prediction}, has been proposed towards addressing above incentive challenges \cite{prelec2004bayesian,gneiting2007strictly,jurca2009mechanisms,witkowski2012peer,radanovic2013robust,dasgupta2013crowdsourced}. Existing peer prediction mechanisms focus on achieving incentive compatibility (IC), which is defined as reporting truthfully a private data, or reporting a high quality data, will maximize the expected payment issued to workers. They achieve IC via comparing the reports between the targeted worker, and a randomly selected reference worker, to bypass the challenge of no ground-truth verification.

Nonetheless, we note several undesirable properties of existing peer prediction mechanisms.
Firstly, from the label inference studies~\cite{zheng2017truth}, we can know that all the collected labels are correlated and this correlation contains a wealth of information about the true labels and the quality of workers.
However, existing peer prediction mechanisms purely rely on the reports of the reference worker, which only represents a limited share of the overall collected information.
This way of design will inevitably lower the robustness, and meanwhile increase the variance of payment (which is unfavorable in practice.)
Secondly, existing peer prediction mechanisms simplify workers' responses to the incentive mechanism by assuming that workers are all fully rational and only follow the utility-maximizing strategy.
However, there have been many studies showing that human agents may be bounded-rational and even keep improving their responding strategies in practice~\cite{simon1982models,mckelvey1995quantal,chastain2014algorithms}.




Currently, the connection between ML and incentive mechanism design is far from being satisfactory. In this paper, we propose a \emph{learning-based incentive mechanism}, aiming to merge and extend the techniques in the two areas to address the caveats when they are employed alone, as discussed above.
The high level idea is as follows: we divide the large dataset into relatively small task packages. At each time step, we employ workers to handle one task package and use a certain inference algorithm to learn the true labels and workers' accuracy in their reports. Then, we use the learned worker accuracy o determine the payments for workers. Meanwhile, driving in the background, we develop a reinforcement learning algorithm to adjust the payments based on workers' historical responses to incentivizes. By doing so, our incentive mechanism can be adapted to different types of workers.



However, as the first work to combine the label inference and reinforcement learning algorithms with incentive mechanism design, there is a line of challenges. In light of the challenges, we summarize the core contributions of this paper as follows:
\begin{itemize}
\item In order to achieve good incentive property and calibrate the accuracy workers' reported information (for training our RL algorithm), there is a need of having an unbiased inference algorithm. Unfortunately this is not the case with existing inference method. We propose a novel one-shot peer prediction mechanism based on Bayesian inference by firstly deriving the explicit posterior distribution of true labels and then employing Gibbs sampling for inference. The most challenging problem of our mechanism is to prove the convergence of above inference method.
\item Since workers' states and label accuracy both are unobservable, we need to estimate them via the label inference algorithm. However, these estimates, which are the means over all tasks, are approximately corrupted with Gaussian noise. Thus, we develop our reinforcement learning algorithm based on the data-driven Gaussian process regression.
\end{itemize}
Besides, we conduct empirical evaluation, and the results show that our mechanism can improve the robustness and lower the variance of payments. Meanwhile, in the long term, our mechanism significantly increase the utility of the data requester under different worker models, such as fully rational, bounded rational and learning agent models. 

