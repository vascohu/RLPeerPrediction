\section{Introduction}
The ability to quickly collect large scale and high quality labeled datasets is crucial for Machine Learning (ML), and more generally for Artificial Intelligence. Among all proposed solutions, one of the most promising ones is crowdsourcing \cite{Howe2006,slivkins2014online,difallah2015dynamics,simpson2015language}. The idea is neat - instead of using a centralized amount of efforts, the to-be-labeled tasks are disseminated to a decentralized crowd of workers to parallelize the collection procedure, leveraging the power of human computation. Nonetheless, it has been noted that crowdsourced data often suffers from quality issues, due to its salient feature of no monitoring and no ground-truth verification of workers' contributed data. %cite{waggoner2014output}. 

This quality control challenge has been attempted by two relatively disconnected research communities. From the more ML side, quite a few inference techniques have been developed for inferring true labels from crowdsourced and potentially noisy labels \citep{raykar2010learning,liu2012variational,zhou2014aggregating,zheng2017truth}. These solutions often work as one-shot, post-processing procedures facing a static set of workers, whose labeling accuracy is fixed and \emph{informative}. Despite their empirical success, the above methods ignored the effects of \emph{incentives} when dealing with human inputs. It has been observed both in theory and practice that, without appropriate incentive, selfish and rational workers can easily choose to contribute low quality, uninformative, or even malicious data~\citep{sheng2008get,liu2017sequential}. Existing inference algorithms are very vulnerable in these cases - either much more redundant labels will be needed (low quality inputs), or the methods will simply fail to work (the case with uninformative and malicious inputs). 

From the less ML side, the above quality control question has been studied in the context of \emph{incentive mechanism design}. In particular, a family of mechanisms, jointly called \emph{peer prediction}, has been proposed towards addressing the above incentive challenges \cite{prelec2004bayesian,jurca2009mechanisms,witkowski2012peer,dasgupta2013crowdsourced}. Existing peer prediction mechanisms focus on achieving incentive compatibility (IC), which is defined as truthfully reporting private data, or reporting high quality data, will maximize workers' expected utilities. These mechanisms achieve IC via comparing the reports between the targeted worker, and a randomly selected reference worker, to bypass the challenge of no ground-truth verification.
Nonetheless, we note several undesirable properties of existing peer prediction mechanisms.
Firstly, from the label inference studies~\cite{zheng2017truth}, we know that the collected labels contain a wealth of information about the true labels and workers' labeling accuracy. Nonetheless, existing peer prediction mechanisms often rely on the labels of a small set of reference workers, which only represents a limited share of the overall collected information.
%This way of design will inevitably lower the robustness, and meanwhile increase the variance of payment (which is unfavorable in practice.)
Secondly, existing peer prediction mechanisms simplify workers' responses to the incentive mechanism by assuming that workers are all fully rational and only follow the utility-maximizing strategy.
However, there are evidences showing that human agents may follow bounded-rationality model, and may improve their responding strategies in practice~\cite{simon1982models,chastain2014algorithms,gao2014trick}. Lastly, it is often assumed that workers' costs in exerting effort to produce high quality labels are known by the mechanism designer. 




In this paper, we propose a \emph{learning-based incentive mechanism}, aiming to merge and extend the techniques in the two disconnected areas to address the caveats when they are employed alone, as discussed above.
The high level idea is as follows: we divide the large to-be-collected dataset into relatively small task packages. At each step, we employ workers to handle one task package and estimate the true labels and workers' strategies from their reports.
Relying on the above estimates, a reinforcement learning (RL) algorithm is used to uncover how workers respond to different levels of offered payments.
We determine the payments for workers based on workers' current strategies and the output of the RL algorithm.
By doing so, our mechanism not only incentivizes rational workers to provide high-quality labels but also dynamically adjusts the payments according to workers' types.

% Then, we use the learned worker accuracy o determine the payments for workers. Meanwhile, driving in the background, we develop a reinforcement learning algorithm to adjust the payments based on workers' historical responses to incentivizes. By doing so, our incentive mechanism can be adapted to different types of workers.



We summarize our core contributions as follows:
\begin{itemize}[topsep=0pt,  itemsep=0pt]
\item We propose an incentive compatible RL framework to (i) incentivize high quality labels from workers and (ii) maximize data requester's utility, without assuming any agent model for workers.  
\item We provide a novel method to prove the long-term incentive compatibility of reinforcement learning algorithms.
\item To achieve incentive compatibility and to calibrate the estimates of label accuracy for training our RL algorithm, we develop a novel Bayesian inference algorithm and theoretically prove its convergence. Since the inference results are approximately corrupted with Gaussian noise, we develop a RL algorithm based on the data-driven Gaussian process regression. 
\end{itemize}
Besides, we conduct extensive experiments of our algorithms, and the results show that our Bayesian inference algorithm can improve the robustness and lower the variance of payments, which is practically favorable. Meanwhile, our RL algorithm can significantly increase the utility of the data requester under different worker models, such as fully rational and learning agent models. 

