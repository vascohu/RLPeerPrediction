\section{Introduction}
The ability to quickly collect large scale and high quality labeled datasets is crucial for Machine Learning (ML), and more generally for Artificial Intelligence. Among all proposed solutions, one of the most promising ones is crowdsourcing \cite{slivkins2014online,difallah2015dynamics,simpson2015language}. The idea is neat - instead of using a centralized amount of efforts, the to-be-labeled tasks are disseminated to a decentralized crowd of workers to parallelize the collection procedure, leveraging the power of human computation. Nonetheless, it has been noted that crowdsourced data often suffers from quality issues, due to its unique feature of no monitoring and no ground-truth verification of workers' contributed data. %cite{waggoner2014output}. 

This quality control challenge has been attempted by two relatively disconnected research communities. From the more ML side, quite a few inference techniques have been developed for inferring true labels from crowdsourced and potentially noisy labels \citep{zheng2017truth}. These solutions often work as a one-shot, post-processing procedure facing a static set of workers, whose labeling accuracy is fixed and \emph{informative}. Despite their empirical success, the above methods ignored the effects of \emph{incentives} when dealing with human inputs. It has been observed both in theory and practice that, without appropriate incentive, selfish and rational workers can easily chose to contribute low quality, uninformative, or even malicious data~\cite{liu2017sequential}. Existing inference algorithms are very vulnerable in these cases - either much more redundant labels will be needed (low quality inputs), or the methods will simply fail to work (the case with uninformative and malicious inputs). 

From the less ML side, the above quality control question has been studied in the context of \emph{incentive mechanism design}. In particular, a family of mechanisms, jointly called \emph{peer prediction}, has been proposed towards addressing above incentive challenges \cite{dasgupta2013crowdsourced}. Existing peer prediction mechanisms focus on achieving incentive compatibility (IC), which is defined as reporting truthfully a private data, or reporting a high quality data, will maximize workers' expected utilities. These mechanisms achieve IC via comparing the reports between the targeted worker, and a randomly selected reference worker, to bypass the challenge of no ground-truth verification.

Nonetheless, we note several undesirable properties of existing peer prediction mechanisms.
Firstly, from the label inference studies~\cite{zheng2017truth}, we can know that all the collected labels contain a wealth of information about the true labels and workers' states.
However, existing peer prediction mechanisms purely rely on the labels of the reference worker, which only represents a limited share of the overall collected information.
%This way of design will inevitably lower the robustness, and meanwhile increase the variance of payment (which is unfavorable in practice.)
Secondly, existing peer prediction mechanisms simplify workers' responses to the incentive mechanism by assuming that workers are all fully rational and only follow the utility-maximizing strategy.
However, there have been many studies showing that human agents may be bounded-rational and even keep improving their responding strategies in practice~\cite{simon1982models,mckelvey1995quantal,chastain2014algorithms}.




In this paper, we propose a \emph{learning-based incentive mechanism}, aiming to merge and extend the techniques in the two disconnected areas to address the caveats when they are employed alone, as discussed above.
The high level idea is as follows: we divide the large dataset into relatively small task packages. At each time step, we employ workers to handle one task package and infer the true labels and workers' states from their reports.
Meanwhile, a reinforcement learning (RL) algorithm is used to mine the connection between workers' state changes and the incentives.
We determines the payments for workers based on the inference results and the output of the RL algorithm.
By doing so, we can fully exploit the collected labels, adapt to different types of workers and induce workers to generate high-quality labels.

% Then, we use the learned worker accuracy o determine the payments for workers. Meanwhile, driving in the background, we develop a reinforcement learning algorithm to adjust the payments based on workers' historical responses to incentivizes. By doing so, our incentive mechanism can be adapted to different types of workers.



However, as the first work to merge these techniques, there is a line of challenges. In light of the challenges, we summarize our core contributions as follows:
\begin{itemize}[topsep=0pt,  itemsep=0pt]
\item To support the IC of our mechanism and calibrate the estimates of label accuracy for training our RL algorithm, we develop a novel Bayesian inference algorithm and theoretically prove its convergence.
\item Since the inference results are approximately corrupted with Gaussian noise, we develop a RL algorithm based on the data-driven Gaussian process regression. Meanwhile, we provide a novel method to prove the long-term IC of reinforcement learning algorihtms.
\end{itemize}
Besides, we conduct extensive experiments, and the results show that our Bayesian inference algorithm can improve the robustness and lower the variance of payments. Meanwhile, our RL algorithm can significantly increase the utility of the data requester under different worker models, such as fully rational, bounded rational and learning agent models. 

