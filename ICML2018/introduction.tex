\section{Introduction}
The ability to quickly collect large scale and high quality labeled datasets is crucial for Machine Learning, and more generally Artificial Intelligence. Among all proposed solutions, one of the most promising ones is crowdsourcing \cite{} [cite AMT, imagenet]. The idea is neat - instead of using a centralized amount of efforts, the labeling tasks will be disseminated to a decentralized crowd of workers, leveraging the power of human computation, to parallelize this collection procedure. Nonetheless, it has been noted not after a long time of this proposal that, crowdsourced data often suffered from quality issues, due to its unique feature of no monitoring and no ground-truth verification of workers' contributed data. 

The above quality control challenge has been attempted to resolve by two rather disconnected research communities separately. From the more Machine Learning side, a family of variational and Bayesian inference techniques developed for inferring true labels from crowdsourced and potentially noisy labels \cite{}, often working as a one-shot, post-processing procedure facing a static set of workers, whose labeling accuracy is fixed and \emph{informative}. Despite its nice theoretical contribution and empirical success, the above methods ignored the effects of \emph{incentives} when dealing with human inputs. It has been observed both in theory and practice that \cite{}, without appropriate incentive, selfish and rational workers can easily chose to contribute low quality, uninformative, or even malicious data. Existing inferences are very vulnerable in these cases - either much more redundant crowdsourced labels will need to be collected (low quality inputs), or the methods will simply fail to work (the case with uninformative and malicious inputs). 

The above data quality control question has been studied in the context of \emph{incentive design}, mostly in a non-ML setting (except for \cite{cite my EC'17 paper}). In particular, a family of mechanisms, jointly called \emph{peer prediction}, has been proposed towards addressing above incentive challenges \cite{}. Existing peer prediction mechanisms focus on achieving incentive compatibility (IC) defined as the reporting a high quality data will maximizes the expected payment issued to workers. The idea is to achieve IC via comparing the reports between the targeted reported data, and a randomly selected reference answer. We note several undesirable properties of peer prediction, which we suspect are the reasons that lead to its inconsistent performance in practice \cite{}. First, existing peer prediction mechanisms simplify workers' responses to the incentive mechanism by assuming that workers are all fully rational and only follow the utility-maximizing strategy. However, there is evidence showing that human workers are not always fully rational, and they may deviate from equilibrium strategies. Thus, these peer prediction mechanisms that are fancy in theory may yet fail in practice \cite{}. Secondly, existing mechanisms are not prior-free, meaning that workers' cost in exerting effort to produce high quality data is assumed to be known - this is unlikely to be the case in practice. Thirdly, we find that some other properties of incentives, such as variance of payment, robustness to the environments etc, are often overlooked, as well as the variation of incentives among different types of agents - we suspect partly this is because the existing methods only use a small amount of all collected labels.



We believe the above disconnection is far from being satisfactory and unnecessary. In this paper, we propose an incentive compatible \emph{reinforcement Bayesian inference} approach, aiming to marry and extend the above two areas and techniques, and to address the existing caveats in above incentive design and inference learning methods in a sequential setting. We chose to go with a sequentially interactive setting, not only because this is often the case in practice, but also we find this setting is extremely suitable for us to leverage reinforcement learning type of sequential learning algorithm to relax our algorithm to an entirely parameter- and model-free setting. 

The high level idea is as follows: at each step, we will leverage the power of Bayesian inference method to design an incentive mechanism to induce high effort from workers. Driving in the background, we develop a reinforcement learning algorithm to sequentially interact with agents to learn their best incentive levels. 

Addressing above challenges are not technically easy. There is a line of challenges that need to be addressed. We list several key ones here:
\begin{itemize}
\item In order to achieve good incentive property and calibrate the accuracy workers' reported information, there is a need of having an unbiased inference algorithm. Unfortunately this is not the case with existing inference method. %Also we need to prove the convergence of such an inference algorithm. 
\item In order to learn the right incentive for each agent, we will leverage reinforcement learning as the underlying sequentially interactive algorithm. Nonetheless
\begin{itemize}
\item The state of RL is missing: due to unobservable workers' work state;
\item The reward state, i.e., how well workers respond to each level of offered payment is unbservable: this is again due to no ground-truth, as well as the unobservability of workers' state.
\item Possibility of long term manipulation: agents can hope to contribute malicious data in hope of a long term return. 
\end{itemize}
\end{itemize}


We summarize two core contributions in this paper:
\begin{itemize}
\item We propose a novel one-shot peer prediction mechanism based on Bayesian inference. Since existing Bayesian inference algorithms (e.g. EM estimator and variational inference) for crowdsourcing are biased in principle, we derive the explicit posterior distribution of the true labels and employ Gibbs sampling for inference. The most challenging problem of our mechanism is to prove the convergence of above inference method, and further the incentive compatibility of our mechanism which has never been explored in the literature. 

We prove our method can calibrate the accuracy of each agent's reported works, and thus provides strong incentives for workers to exert effort and report truthfully their data. Among all advantages, we note that our method is robust to the existence of low quality but highly correlating signals (e.g., essay length in the peer grading case). Besides, we also empirically show the advantages of our mechanism on the stability and fairness of incentives over existing ones.
\item We design the first reinforcement Bayesian inference framework which sequentially interacts with workers, addressing the issues of unobservable states and rewards. It dynamically adjusts the scaling level of our incentive mechanism to maximize the utility of the data requester, as well as to estimate the true states and rewards for each corresponding actions. To avoid assuming a decision-making model for workers, we use the data-driven Gaussian process to represent the scaling level adjustment policy, and online updates our policy according to workers' responses. 

We prove a sufficient condition under which the reinforcement learning algorithm we built is robust to long term manipulation of each agent too. Thus we are able to avoid the case where agents are not greedy (or often referred to as simple agents in the literature) and are strategic in reasoning long term returns. This robust to manipulation result only shows the benefits of our reinforcement learning algorithm. We also empirically show its advantages on improving the utility if the data requester over one-shot mechanisms.
\end{itemize}

The rest of the paper organizes as follows: XXXX.